{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches,  lines\n",
    "import time\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "# To find local version of the library\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from humanware.svhn.utils import load_obj\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_dir = 'data/SVHN/train/'\n",
    "extradata_dir = 'data/SVHN/extra/'\n",
    "\n",
    "filename = 'labels'\n",
    "metadata_train = load_obj(traindata_dir, filename)\n",
    "metadata_extra = load_obj(extradata_dir, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataloader.\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SVHNDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (dict): Dictionary containing all labels and metadata\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.metadata = metadata\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The index of the dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : PIL objet\n",
    "        \n",
    "        y : dict\n",
    "            The metadata associated to the image in dict form.\n",
    "\n",
    "        '''\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        img_name = os.path.join(self.data_dir,\n",
    "                                self.metadata[index]['filename'])\n",
    "        \n",
    "        # Load data and get raw metadata (labels & boxes)\n",
    "        image = Image.open(img_name)\n",
    "        metadata_raw = self.metadata[index]['metadata']\n",
    "\n",
    "        labels, boxes = extract_labels_boxes(metadata_raw)\n",
    "        \n",
    "        metadata = {'labels': labels, 'boxes': boxes}\n",
    "        \n",
    "        sample = {'image': image, 'metadata': metadata}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstCrop(object):\n",
    "    \"\"\"Crop the image such that all bounding boxes +30% in x,y are contained in the image.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_size):\n",
    "\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image = sample['image']\n",
    "        \n",
    "        labels, boxes = sample['metadata']['labels'], sample['metadata']['boxes'] \n",
    "\n",
    "        outer_box = extract_outer_box(sample, padding=self.pad_size)\n",
    "        outer_box = np.round(outer_box).astype('int')\n",
    "        \n",
    "        x1_tot, x2_tot, y1_tot, y2_tot = outer_box\n",
    "\n",
    "        boxes_cropped = boxes\n",
    "        boxes_cropped[:,0:2] -= x1_tot\n",
    "        boxes_cropped[:,2:] -= y1_tot\n",
    "\n",
    "        img_cropped = image.crop((x1_tot,y1_tot,x2_tot,y2_tot))\n",
    "        \n",
    "        metadata = {'boxes': boxes_cropped, 'labels': labels}\n",
    "        \n",
    "        return {'image': img_cropped, 'metadata': metadata}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "      \n",
    "        h, w = np.asarray(image).shape[:2]\n",
    "\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image_scaled = image.resize((new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        \n",
    "        boxes = boxes.astype('float64')\n",
    "        boxes[:,:2] *= (new_w / w)\n",
    "        boxes[:, 2:] *= (new_h / h)\n",
    "        boxes = boxes.astype('int64')\n",
    "        \n",
    "        metadata = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        return {'image': image_scaled, 'metadata': metadata}\n",
    "\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "\n",
    "        h, w = np.asarray(image).shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        \n",
    "        image_cropped = image.crop((left,top,left+new_w,top+new_h))\n",
    "\n",
    "        boxes[:, 0:2] -= left\n",
    "        boxes[:, 2:] -= top\n",
    "                \n",
    "        boxes[:, :2] = np.clip(boxes[:, :2], 0, new_w-1)\n",
    "        boxes[:, 2:] = np.clip(boxes[:, 2:], 0, new_h-1)\n",
    "        \n",
    "        metadata = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        return {'image': image_cropped, 'metadata': metadata}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        image = image - np.mean(image)\n",
    "        assert image.shape == (54, 54, 3)\n",
    "        \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        \n",
    "        # TODO\n",
    "        # Process boxes\n",
    "        \n",
    "        labels = np.asarray(labels)\n",
    "        \n",
    "        # Target is a 1x6 vector, where [0] is the number of digits and \n",
    "        # targets[1:targets[0]] is the digit sequence.\n",
    "        # i.e. the sequence 157 is represented by target [3,1,5,5,7,-1,-1]\n",
    "        target = -np.ones(6)\n",
    "        target[0] = len(labels)\n",
    "\n",
    "        for jj in range(len(labels)):\n",
    "\n",
    "            target[jj+1] = labels[jj]\n",
    "            \n",
    "        target = torch.from_numpy(target).int()        \n",
    "#         metadata = {'boxes': boxes, 'labels': labels}\n",
    "        \n",
    "        return {'image': image,\n",
    "                'target': target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## code to extract bboxes\n",
    "## Inspiration: https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/visualize.py\n",
    "\n",
    "def extract_labels_boxes(meta):\n",
    "    '''Extract the labels and boxes from a given sample'''\n",
    "    \n",
    "#     meta = sample['metadata']\n",
    "    N = len(meta['label']) # Number of digits in image\n",
    "\n",
    "    labels = [] # Digits present in image\n",
    "    boxes = [] # bboxes present in image\n",
    "\n",
    "    # Extract digit boxes and labels\n",
    "    for jj in range(N):\n",
    "        labels.append(meta['label'][jj])\n",
    "        y1 = meta['top'][jj]\n",
    "        y2 = y1+meta['height'][jj]\n",
    "        x1 = meta['left'][jj]\n",
    "        x2 = x1 + meta['width'][jj]\n",
    "\n",
    "        boxes.append((x1,x2,y1,y2))\n",
    "        \n",
    "    boxes = np.asarray(boxes)\n",
    "        \n",
    "    return labels, boxes\n",
    "\n",
    "\n",
    "def extract_outer_box(sample, padding=0.3):\n",
    "    \n",
    "    img_shape = np.asarray(sample['image']).shape\n",
    "    boxes = sample['metadata']['boxes']\n",
    "\n",
    "    x1_tot = np.min(boxes[:,0])\n",
    "    x2_tot = np.max(boxes[:,1])\n",
    "    y1_tot = np.min(boxes[:,2])\n",
    "    y2_tot = np.max(boxes[:,3])\n",
    "    \n",
    "    x1_tot -= padding/2*(x2_tot-x1_tot)\n",
    "    x2_tot += padding/2*(x2_tot-x1_tot)\n",
    "    y1_tot -= padding/2*(y2_tot-y1_tot)\n",
    "    y2_tot += padding/2*(y2_tot-y1_tot)\n",
    "    \n",
    "    x1_tot = max(0, x1_tot)\n",
    "    x2_tot = min(x2_tot, img_shape[1]-1)\n",
    "    y1_tot = max(0, y1_tot)\n",
    "    y2_tot = min(y2_tot, img_shape[0]-1)\n",
    "    \n",
    "    \n",
    "    outer_bbox = (x1_tot, x2_tot, y1_tot, y2_tot)\n",
    "        \n",
    "    return outer_bbox\n",
    "\n",
    "    \n",
    "def visualize_sample(sample, outer_bbox=None):\n",
    "    \n",
    "    img = sample['image']\n",
    "    boxes = sample['metadata']['boxes']    \n",
    "    labels = sample['metadata']['labels']    \n",
    "\n",
    "    # Display image\n",
    "    _, ax = plt.subplots(1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    \n",
    "    N = len(labels) # Number of digits in image\n",
    "    \n",
    "    # Show individual boxes and labels\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Show bounding boxes\n",
    "        c = ['r','k']\n",
    "        if boxes is not None:\n",
    "            x1, x2, y1, y2 = boxes[i]\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                alpha=0.7, linestyle=\"dashed\",\n",
    "                                edgecolor=c[i % 2], facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            # Show Label\n",
    "            caption = labels[i]\n",
    "            ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "    if outer_bbox is not None:\n",
    "\n",
    "        x1_tot, x2_tot, y1_tot, y2_tot = outer_bbox\n",
    "\n",
    "        p2 = patches.Rectangle((x1_tot, y1_tot), x2_tot - x1_tot, y2_tot - y1_tot, linewidth=2,\n",
    "                            alpha=0.7, linestyle=\"dashed\",\n",
    "                            edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "traindata = SVHNDataset(metadata_train, traindata_dir)\n",
    "\n",
    "firstcrop = FirstCrop(0.3)\n",
    "rescale = Rescale((64, 64))\n",
    "random_crop = RandomCrop((54, 54))\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "transform = transforms.Compose([firstcrop,\n",
    "                                rescale,\n",
    "                                random_crop,\n",
    "                                ])\n",
    "\n",
    "index = np.random.randint(len(traindata))\n",
    "print(\"Index: \", index)\n",
    "sample = traindata[index] \n",
    "visualize_sample(sample)\n",
    "\n",
    "for i, tsfrm in enumerate([firstcrop, rescale, random_crop]):\n",
    "    \n",
    "    sample = tsfrm(sample)\n",
    "    visualize_sample(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "firstcrop = FirstCrop(0.3)\n",
    "rescale = Rescale((64, 64))\n",
    "random_crop = RandomCrop((54, 54))\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([firstcrop,\n",
    "                                rescale,\n",
    "                                random_crop,\n",
    "                                to_tensor])\n",
    "\n",
    "transformed_dataset = SVHNDataset(metadata_train, data_dir=traindata_dir, transform=transform)\n",
    "\n",
    "\n",
    "indices = np.arange(len(metadata_train))\n",
    "indices = np.random.permutation(indices)\n",
    "\n",
    "\n",
    "train_idx = indices[:round(0.8*len(indices))]\n",
    "valid_idx = indices[round(0.8*len(indices)):]\n",
    "sample_idx = indices[:100]\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "sample_sampler = torch.utils.data.SubsetRandomSampler(sample_idx)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=4, \n",
    "                          sampler=train_sampler)\n",
    "\n",
    "valid_loader = DataLoader(transformed_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=4, \n",
    "                          sampler=valid_sampler)\n",
    "\n",
    "sample_loader = DataLoader(transformed_dataset, \n",
    "                           batch_size=batch_size, \n",
    "                           shuffle=False, \n",
    "                           num_workers=4, \n",
    "                           sampler=sample_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a placeholder CNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BaselineCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7744, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 7)\n",
    "     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1) # Flatten based on batch size\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cnn = BaselineCNN()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, device, num_epochs=10, lr=0.001, model_out=None):\n",
    "\n",
    "    since = time.time()\n",
    "    model.to(device)\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_ndigits = CrossEntropyLoss()\n",
    "\n",
    "    print(\"# Start training #\")\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0\n",
    "        train_n_iter = 0\n",
    "\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over train data\n",
    "        for i, batch in enumerate(sample_loader):\n",
    "            # get the inputs\n",
    "            inputs, targets = batch['image'], batch['target'] \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            target_ndigits = targets[:,0].long()\n",
    "\n",
    "            target_ndigits.to(device)\n",
    "\n",
    "\n",
    "            # Zero the gradient buffer\n",
    "            optimizer.zero_grad()  \n",
    "\n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_ndigits(outputs, target_ndigits)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            train_n_iter += 1\n",
    "\n",
    "        valid_loss = 0\n",
    "        valid_n_iter = 0\n",
    "\n",
    "        # Set model to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        # Iterate over valid data\n",
    "        # Iterate over train data\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            # get the inputs\n",
    "            inputs, targets = batch['image'], batch['target'] \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            target_ndigits = targets[:,0].long()\n",
    "            target_ndigits.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_ndigits(outputs, target_ndigits)\n",
    "\n",
    "            # Statistics\n",
    "            valid_loss += loss.item()\n",
    "            valid_n_iter += 1\n",
    "\n",
    "        train_loss_history.append(train_loss / train_n_iter)\n",
    "        valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "\n",
    "        print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "        print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    if model_out:\n",
    "        print('Saving model ...')\n",
    "        torch.save(model, model_out)\n",
    "        print('Model saved to :', model_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = \"models/my_model.pth\"\n",
    "\n",
    "train_model(baseline_cnn, train_loader=sample_loader, \n",
    "            valid_loader=sample_loader, \n",
    "            device=device, model_out=model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get useful insight into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits_hist = np.zeros(7)\n",
    "\n",
    "n_digits_train = 0\n",
    "n_digits_extra = 0\n",
    "\n",
    "for ii in metadata_train:\n",
    "    \n",
    "    n_digits = int(len(metadata_train[ii]['metadata']['label']))\n",
    "    n_digits_train += n_digits\n",
    "    if n_digits < 6:\n",
    "        n_digits_hist[n_digits] += 1\n",
    "    else:\n",
    "        n_digits_hist[6] += 1\n",
    "        \n",
    "    \n",
    "for ii in metadata_extra:\n",
    "    \n",
    "    n_digits = int(len(metadata_extra[ii]['metadata']['label']))\n",
    "\n",
    "    n_digits_extra += n_digits\n",
    "    \n",
    "    if n_digits < 6:\n",
    "        n_digits_hist[n_digits] += 1\n",
    "    else:\n",
    "        n_digits_hist[6] += 1\n",
    "\n",
    "\n",
    "\n",
    "print('total number of digits: ', n_digits_train + n_digits_extra)\n",
    "print('total number of sequences', sum(n_digits_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(n_digits_hist)), n_digits_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for ii, jj in enumerate(n_digits_hist):\n",
    "    tot += ii*jj\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smallest dimensions of images possible\n",
    "# Slow since you have to load every image into memory\n",
    "\n",
    "im_width = []\n",
    "im_height =  []\n",
    "for jj in range(len(traindata)):\n",
    "    \n",
    "    shape = np.asarray(traindata[jj]['image']).shape\n",
    "    im_height.append(shape[0])\n",
    "    im_width.append(shape[1])\n",
    "    \n",
    "im_width = np.asarray(im_width)\n",
    "im_height = np.asarray(im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset for cleaning\n",
    "\n",
    "# Minimum width and height of images\n",
    "print(\"minimum image width\", np.min(im_width))\n",
    "print(\"minimum image height\", np.min(im_height))\n",
    "\n",
    "\n",
    "#\n",
    "total = np.sum(np.logical_or(im_height < 28, im_width < 28))\n",
    "\n",
    "print('total number of image in dataset: ', len(traindata))\n",
    "print('total number of images that are too small', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show sample image that is too small\n",
    "\n",
    "index = np.argmin(im_height)\n",
    "visualize_sample(traindata, idx=index)\n",
    "\n",
    "sample = traindata[index]\n",
    "print(sample['metadata']['label'])\n",
    "np.asarray(sample['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example of at least one transform\n",
    "# use imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-id",
   "language": "python",
   "name": "text-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
