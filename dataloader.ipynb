{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches,  lines\n",
    "import time\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "from humanware.svhn.utils import load_obj\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/SVHN/train/'\n",
    "filename = 'labels'\n",
    "metadata = load_obj(data_dir, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataloader.\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SVHNDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (dict): Dictionary containing all labels and metadata\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.metadata = metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The index of the dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : PIL objet\n",
    "        \n",
    "        y : dict\n",
    "            The metadata associated to the image in dict form.\n",
    "\n",
    "        '''\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.metadata[index]['filename'])\n",
    "\n",
    "        # Load data and get labels\n",
    "        image = Image.open(img_name)\n",
    "        meta = self.metadata[index]['metadata']\n",
    "        \n",
    "\n",
    "        \n",
    "        sample_raw = {'image':image, 'metadata':meta}\n",
    "\n",
    "        labels, boxes = extract_labels_boxes(sample_raw)\n",
    "        \n",
    "        metadata = {'labels': labels, 'boxes': boxes}\n",
    "\n",
    "        return {'image': image, 'metadata': metadata}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstCrop(object):\n",
    "    \"\"\"Crop the image such that all bounding boxes +30% in x,y are contained in the image.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_size):\n",
    "\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image = sample['image']\n",
    "        \n",
    "        labels, boxes = sample['metadata']['labels'], sample['metadata']['boxes'] \n",
    "\n",
    "        outer_box = extract_outer_box(sample, padding=self.pad_size)\n",
    "        outer_box = np.round(outer_box).astype('int')\n",
    "        \n",
    "        x1_tot, x2_tot, y1_tot, y2_tot = outer_box\n",
    "\n",
    "        boxes_cropped = boxes\n",
    "        boxes_cropped[:,0:2] -= x1_tot\n",
    "        boxes_cropped[:,2:] -= y1_tot\n",
    "\n",
    "        img_cropped = image.crop((x1_tot,y1_tot,x2_tot,y2_tot))\n",
    "        \n",
    "        metadata = {'boxes': boxes_cropped, 'labels': labels}\n",
    "        \n",
    "        return {'image': img_cropped, 'metadata': metadata}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "      \n",
    "        h, w = np.asarray(image).shape[:2]\n",
    "\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image_scaled = image.resize((new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        \n",
    "        boxes = boxes.astype('float64')\n",
    "        boxes[:,:2] *= (new_w / w)\n",
    "        boxes[:, 2:] *= (new_h / h)\n",
    "        boxes = boxes.astype('int64')\n",
    "        \n",
    "        metadata = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        return {'image': image_scaled, 'metadata': metadata}\n",
    "\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "\n",
    "        h, w = np.asarray(image).shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        \n",
    "        image_cropped = image.crop((left,top,left+new_w,top+new_h))\n",
    "\n",
    "        boxes[:, 0:2] -= left\n",
    "        boxes[:, 2:] -= top\n",
    "                \n",
    "        boxes[:, :2] = np.clip(boxes[:, :2], 0, new_w-1)\n",
    "        boxes[:, 2:] = np.clip(boxes[:, 2:], 0, new_h-1)\n",
    "        \n",
    "        metadata = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        return {'image': image_cropped, 'metadata': metadata}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['metadata']['boxes'], sample['metadata']['labels']\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Draft code to extract bboxes\n",
    "## Inspiration: https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/visualize.py\n",
    "\n",
    "def extract_labels_boxes(sample):\n",
    "    '''Extract the labels and boxes from a given sample'''\n",
    "    \n",
    "    meta = sample['metadata']\n",
    "    N = len(meta['label']) # Number of digits in image\n",
    "\n",
    "    labels = [] # Digits present in image\n",
    "    boxes = [] # bboxes present in image\n",
    "\n",
    "    # Extract digit boxes and labels\n",
    "    for jj in range(N):\n",
    "        labels.append(meta['label'][jj])\n",
    "        y1 = meta['top'][jj]\n",
    "        y2 = y1+meta['height'][jj]\n",
    "        x1 = meta['left'][jj]\n",
    "        x2 = x1 + meta['width'][jj]\n",
    "\n",
    "        boxes.append((x1,x2,y1,y2))\n",
    "        \n",
    "    boxes = np.asarray(boxes)\n",
    "        \n",
    "    return labels, boxes\n",
    "\n",
    "\n",
    "def extract_outer_box(sample, padding=0.3):\n",
    "    \n",
    "    img_shape = np.asarray(sample['image']).shape\n",
    "    boxes = sample['metadata']['boxes']\n",
    "    img_shape = np.asarray(image).shape\n",
    "    x1_tot = np.min(boxes[:,0])\n",
    "    x2_tot = np.max(boxes[:,1])\n",
    "    y1_tot = np.min(boxes[:,2])\n",
    "    y2_tot = np.max(boxes[:,3])\n",
    "    \n",
    "    x1_tot -= padding/2*(x2_tot-x1_tot)\n",
    "    x2_tot += padding/2*(x2_tot-x1_tot)\n",
    "    y1_tot -= padding/2*(y2_tot-y1_tot)\n",
    "    y2_tot += padding/2*(y2_tot-y1_tot)\n",
    "    \n",
    "    x1_tot = max(0, x1_tot)\n",
    "    x2_tot = min(x2_tot, img_shape[1]-1)\n",
    "    y1_tot = max(0, y1_tot)\n",
    "    y2_tot = min(y2_tot, img_shape[0]-1)\n",
    "    \n",
    "    \n",
    "    outer_bbox = (x1_tot, x2_tot, y1_tot, y2_tot)\n",
    "        \n",
    "    return outer_bbox\n",
    "\n",
    "    \n",
    "def visualize_sample(sample, outer_bbox=None):\n",
    "    \n",
    "    img = sample['image']\n",
    "    boxes = sample['metadata']['boxes']    \n",
    "    labels = sample['metadata']['labels']    \n",
    "\n",
    "    # Display image\n",
    "    _, ax = plt.subplots(1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    \n",
    "    N = len(labels) # Number of digits in image\n",
    "    \n",
    "    # Show individual boxes and labels\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Show bounding boxes\n",
    "        c = ['r','k']\n",
    "        if boxes is not None:\n",
    "            x1, x2, y1, y2 = boxes[i]\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                alpha=0.7, linestyle=\"dashed\",\n",
    "                                edgecolor=c[i % 2], facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            # Show Label\n",
    "            caption = labels[i]\n",
    "            ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "    if outer_bbox is not None:\n",
    "\n",
    "        x1_tot, x2_tot, y1_tot, y2_tot = outer_bbox\n",
    "\n",
    "        p2 = patches.Rectangle((x1_tot, y1_tot), x2_tot - x1_tot, y2_tot - y1_tot, linewidth=2,\n",
    "                            alpha=0.7, linestyle=\"dashed\",\n",
    "                            edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traindata = SVHNDataset(metadata, data_dir)\n",
    "\n",
    "firstcrop = FirstCrop(0.3)\n",
    "rescale = Rescale((64, 64))\n",
    "random_crop = RandomCrop((54, 54))\n",
    "\n",
    "index = np.random.randint(len(traindata))\n",
    "print(\"Index: \", index)\n",
    "sample = traindata[index]\n",
    "\n",
    "image = sample['image']\n",
    "labels, boxes = sample['metadata']['labels'], sample['metadata']['boxes'] \n",
    "\n",
    "outer_box = extract_outer_box(sample)\n",
    "\n",
    "visualize_sample(sample, outer_bbox=outer_box)\n",
    "\n",
    "sample_first_crop = firstcrop(sample)\n",
    "\n",
    "visualize_sample(sample_first_crop)\n",
    "\n",
    "sample_rescaled = rescale(sample_first_crop)\n",
    "\n",
    "visualize_sample(sample_rescaled)\n",
    "\n",
    "sample_randcrop = random_crop(sample_rescaled)\n",
    "\n",
    "visualize_sample(sample_randcrop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smallest dimensions of images possible\n",
    "# Slow since you have to load every image into memory\n",
    "\n",
    "im_width = []\n",
    "im_height =  []\n",
    "for jj in range(len(traindata)):\n",
    "    \n",
    "    shape = np.asarray(traindata[jj]['image']).shape\n",
    "    im_height.append(shape[0])\n",
    "    im_width.append(shape[1])\n",
    "    \n",
    "im_width = np.asarray(im_width)\n",
    "im_height = np.asarray(im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset for cleaning\n",
    "\n",
    "# Minimum width and height of images\n",
    "print(\"minimum image width\", np.min(im_width))\n",
    "print(\"minimum image height\", np.min(im_height))\n",
    "\n",
    "\n",
    "#\n",
    "total = np.sum(np.logical_or(im_height < 28, im_width < 28))\n",
    "\n",
    "print('total number of image in dataset: ', len(traindata))\n",
    "print('total number of images that are too small', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show sample image that is too small\n",
    "\n",
    "index = np.argmin(im_height)\n",
    "visualize_sample(traindata, idx=index)\n",
    "\n",
    "sample = traindata[index]\n",
    "print(sample['metadata']['label'])\n",
    "np.asarray(sample['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example of at least one transform\n",
    "# use imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-id",
   "language": "python",
   "name": "text-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
