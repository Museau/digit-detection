{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches,  lines\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/41176258/h5py-access-data-in-datasets-in-svhn\n",
    "\n",
    "def get_box_data(index, hdf5_data):\n",
    "    \"\"\"\n",
    "    get `left, top, width, height` of each picture\n",
    "    :param index:\n",
    "    :param hdf5_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    meta_data = dict()\n",
    "    meta_data['height'] = []\n",
    "    meta_data['label'] = []\n",
    "    meta_data['left'] = []\n",
    "    meta_data['top'] = []\n",
    "    meta_data['width'] = []\n",
    "\n",
    "    def print_attrs(name, obj):\n",
    "        vals = []\n",
    "        if obj.shape[0] == 1:\n",
    "            vals.append(obj[0][0])\n",
    "        else:\n",
    "            for k in range(obj.shape[0]):\n",
    "                vals.append(int(hdf5_data[obj[k][0]][0][0]))\n",
    "        meta_data[name] = vals\n",
    "\n",
    "    box = hdf5_data['/digitStruct/bbox'][index]\n",
    "    hdf5_data[box[0]].visititems(print_attrs)\n",
    "    return meta_data\n",
    "\n",
    "def get_name(index, hdf5_data):\n",
    "    name = hdf5_data['/digitStruct/name']\n",
    "    return ''.join([chr(v[0]) for v in hdf5_data[name[index][0]].value])\n",
    "\n",
    "\n",
    "def aggregate_data(index, hdf5_data):\n",
    "    \n",
    "    image_id = get_name(index, mat_data)\n",
    "    labels = get_box_data(index, mat_data)\n",
    "    \n",
    "    # Convert label 10 to label 0 for digit 0\n",
    "    if 10 in labels['label']:\n",
    "        labels['label'] = [0 if x==10 else x for x in labels['label']]\n",
    "        \n",
    "    metadata = {}\n",
    "    \n",
    "    metadata['filename'] = image_id\n",
    "    metadata['metadata'] = labels\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_obj(obj, root_dir, filename):\n",
    "    with open(root_dir + filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(root_dir, filename):\n",
    "    with open(root_dir + filename + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data/SVHN/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Download SVHN data in to /data/SVHN/\n",
    "# Parse all metadata from digitStruct.mat into metadata dict (long!)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "file1 = 'data/SVHN/train/digitStruct.mat'\n",
    "mat_data = h5py.File(file1)\n",
    "dataset_size = mat_data['/digitStruct/name'].size\n",
    "\n",
    "\n",
    "metadata = {}\n",
    "for index in range(dataset_size):\n",
    "    \n",
    "    metadata[index] = aggregate_data(index, mat_data)\n",
    "\n",
    "    if index % 5000 == 0:\n",
    "        print(index)\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time :\", end_time - start_time)\n",
    "\n",
    "print(\"Saving metadata dict ...\")\n",
    "filename = 'labels'\n",
    "save_obj(metadata, root_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataloader.\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SVHNDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (dict): Dictionary containing all labels and metadata\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.metadata = metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The index of the dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : PIL objet\n",
    "        \n",
    "        y : dict\n",
    "            The metadata associated to the image in dict form.\n",
    "\n",
    "        '''\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.metadata[index]['filename'])\n",
    "\n",
    "        # Load data and get labels\n",
    "        image = Image.open(img_name)\n",
    "        meta = self.metadata[index]['metadata']\n",
    "        \n",
    "        sample = {'image':image, 'metadata':meta}\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'labels'\n",
    "\n",
    "metadata = load_obj(root_dir, filename)\n",
    "traindata = SVHNDataset(metadata, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Draft code to extract bboxes\n",
    "## Inspiration: https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/visualize.py\n",
    "\n",
    "\n",
    "def visualize_sample(dataset, idx=None, bbox=True, captions=True):\n",
    "    \n",
    "    \n",
    "    # Fetch image + labels    \n",
    "    if not idx:    \n",
    "        idx = np.random.randint(len(dataset))\n",
    "\n",
    "    sample = dataset[idx]\n",
    "    img = sample['image']\n",
    "    meta = sample['metadata']\n",
    "    \n",
    "    # Display image\n",
    "    _, ax = plt.subplots(1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    \n",
    "    \n",
    "    N = len(meta['label']) # Number of digits in image\n",
    "\n",
    "    labels = [] # Digits present in image\n",
    "    boxes = [] # bboxes present in image\n",
    "\n",
    "    # Extract boxes and labels\n",
    "    for jj in range(N):\n",
    "        labels.append(meta['label'][jj])\n",
    "        y1 = meta['top'][jj]\n",
    "        y2 = y1+meta['height'][jj]\n",
    "        x1 = meta['left'][jj]\n",
    "        x2 = x1 + meta['width'][jj]\n",
    "\n",
    "        boxes.append((y1,x1,y2,x2))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Show boxes and labels\n",
    "    for i in range(N):\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        # Show bounding boxes\n",
    "        if bbox:\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                alpha=0.7, linestyle=\"dashed\",\n",
    "                                edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "\n",
    "        # Show Label\n",
    "        if captions:\n",
    "            caption = labels[i]\n",
    "            ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smallest dimensions of images possible\n",
    "\n",
    "im_width = []\n",
    "im_height =  []\n",
    "for jj in range(len(traindata)):\n",
    "    \n",
    "    shape = np.asarray(traindata[jj]['image']).shape\n",
    "    im_height.append(shape[0])\n",
    "    im_width.append(shape[1])\n",
    "    \n",
    "im_width = np.asarray(im_width)\n",
    "im_height = np.asarray(im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset for cleaning\n",
    "\n",
    "# Minimum width and height of images\n",
    "print(\"minimum image width\", np.min(im_width))\n",
    "print(\"minimum image height\", np.min(im_height))\n",
    "\n",
    "\n",
    "#\n",
    "total = np.sum(np.logical_or(im_height < 28, im_width < 28))\n",
    "\n",
    "print('total number of images that are too small', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample image that is too small\n",
    "\n",
    "index = np.argmin(im_height)\n",
    "visualize_sample(traindata, idx=index)\n",
    "\n",
    "sample = traindata[index]\n",
    "print(sample['metadata']['label'])\n",
    "np.asarray(sample['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example of at least one transform\n",
    "# use imgaug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-id",
   "language": "python",
   "name": "text-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
